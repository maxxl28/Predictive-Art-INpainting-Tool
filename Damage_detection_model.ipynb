{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# README:\n",
        "\n",
        "Train a neural network to automatically detect and segment damage. Use Encoder-decoder structure for image segmentation, and attention gates to help the model focus on actual damage areas rather than background noise. Use Focal Tversky loss as it heavily penalizes missed damage and detecting smaller damage areas."
      ],
      "metadata": {
        "id": "sAweXTIVSBxV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t53M76Ysda5_"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import gc\n",
        "import psutil\n",
        "import concurrent.futures\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncRWivySrWyR",
        "outputId": "dc48a494-7a1f-4326-804b-acd3fc92f680"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "TARGET_SIZE = 256  # Reduced to 256 due to memory\n",
        "MAX_WORKERS = 8\n",
        "BATCH_SIZE = 16\n",
        "MAX_SAMPLES = 6000  # Limit to first x images\n",
        "\n",
        "\n",
        "# Setup paths\n",
        "print(\"Setting up data paths\")\n",
        "dataset_dir = \"/content/drive/MyDrive/artwork_data_for_masking2\"\n",
        "damaged_dir = os.path.join(dataset_dir, \"damaged\")\n",
        "mask_dir = os.path.join(dataset_dir, \"masks\")\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Dataset found: {dataset_dir}\")\n",
        "print(f\"Damaged dir: {damaged_dir}\")\n",
        "print(f\"Mask dir: {mask_dir}\")\n",
        "\n",
        "# Find files\n",
        "def find_files(root_dir, suffix):\n",
        "    file_paths = []\n",
        "    for root, _, files in os.walk(root_dir):\n",
        "        for file in files:\n",
        "            if file.endswith(suffix):\n",
        "                rel_path = os.path.relpath(os.path.join(root, file), root_dir)\n",
        "                file_paths.append(rel_path)\n",
        "    return sorted(file_paths)\n",
        "\n",
        "def find_paired_files(damaged_dir, mask_dir):\n",
        "    print(\"Discovering file pairs\")\n",
        "\n",
        "    # Find all damaged images and masks recursively (set up as so there are subfolders based on sub-art movements)\n",
        "    damaged_files = find_files(damaged_dir, '_damaged.jpg')\n",
        "    mask_files = find_files(mask_dir, '_mask.jpg')\n",
        "\n",
        "    print(f\"Found {len(damaged_files)} damaged images\")\n",
        "    print(f\"Found {len(mask_files)} mask images\")\n",
        "    if not damaged_files or not mask_files:\n",
        "        print(f\"No files found. Check paths:\")\n",
        "        print(f\"Damaged: {damaged_dir}\")\n",
        "        print(f\"Masks: {mask_dir}\")\n",
        "        return []\n",
        "\n",
        "    # Pair files by base path\n",
        "    pairs = []\n",
        "    for d in damaged_files:\n",
        "        base = d.replace('_damaged.jpg', '')\n",
        "        matching_masks = [m for m in mask_files if m.replace('_mask.jpg', '') == base]\n",
        "\n",
        "        if not matching_masks:\n",
        "            print(f\"Warning: No mask found for {d}\")\n",
        "            continue\n",
        "        elif len(matching_masks) > 1:\n",
        "            print(f\"Warning: Multiple masks found for {d}\")\n",
        "        category = os.path.dirname(d) if os.path.dirname(d) else \"unknown\"\n",
        "\n",
        "        pairs.append({\n",
        "            'damaged': os.path.join(damaged_dir, d),\n",
        "            'mask': os.path.join(mask_dir, matching_masks[0]),\n",
        "            'category': category,\n",
        "            'name': os.path.basename(base)\n",
        "        })\n",
        "\n",
        "    print(f\"Found {len(pairs)} valid pairs\")\n",
        "    return pairs\n",
        "\n",
        "# Discover all pairs\n",
        "file_pairs = find_paired_files(damaged_dir, mask_dir)\n",
        "if len(file_pairs) == 0:\n",
        "    print(\"No valid pairs found! Check your dataset structure.\")\n",
        "    exit()\n",
        "\n",
        "# Limit number of files for memory purposes\n",
        "if len(file_pairs) > MAX_SAMPLES:\n",
        "    print(f\"Limiting dataset to first {MAX_SAMPLES} samples (from {len(file_pairs)})\")\n",
        "    file_pairs = file_pairs[:MAX_SAMPLES]\n",
        "else:\n",
        "    print(f\"Using all {len(file_pairs)} samples (less than {MAX_SAMPLES})\")\n",
        "\n",
        "# Dataset splitting\n",
        "print(\"\\nMemory optimization\")\n",
        "available_memory_gb = psutil.virtual_memory().available / (1024**3)\n",
        "total_samples = len(file_pairs)\n",
        "\n",
        "# Calculate memory requirements\n",
        "bytes_per_sample = TARGET_SIZE * TARGET_SIZE * 4 * 4  # 4 channels (RGB + mask), 4 bytes per float32\n",
        "estimated_memory_gb = (total_samples * bytes_per_sample) / (1024**3)\n",
        "\n",
        "print(f\"Available memory: {available_memory_gb:.1f} GB\")\n",
        "print(f\"Total samples: {total_samples}\")\n",
        "print(f\"Estimated memory needed: {estimated_memory_gb:.1f} GB\")\n",
        "\n",
        "# Auto-adjust target size if needed\n",
        "if estimated_memory_gb > available_memory_gb * 0.8:\n",
        "    new_target_size = int(TARGET_SIZE * np.sqrt(available_memory_gb * 0.8 / estimated_memory_gb))\n",
        "    TARGET_SIZE = max(128, new_target_size) # Do not go lower than 128 for resolution purposes\n",
        "    print(f\"Reduced target size to {TARGET_SIZE} to fit in memory\")\n",
        "\n",
        "# Group by category for balanced splitting (try to get a diverse set of damaged paintings from each category to train the model)\n",
        "categories = {}\n",
        "for pair in file_pairs:\n",
        "    cat = pair['category']\n",
        "    if cat not in categories:\n",
        "        categories[cat] = []\n",
        "    categories[cat].append(pair)\n",
        "\n",
        "print(f\"Categories found: {list(categories.keys())}\")\n",
        "\n",
        "# Stratified splitting - maintaining same ratios\n",
        "train_pairs = []\n",
        "val_pairs = []\n",
        "test_pairs = []\n",
        "\n",
        "for category, pairs in categories.items():\n",
        "    n = len(pairs)\n",
        "    print(f\"  {category}: {n} samples\")\n",
        "\n",
        "    # Shuffle within category\n",
        "    random.shuffle(pairs)\n",
        "\n",
        "    # Split: 70% train, 15% val, 15% test (same ratios as original)\n",
        "    train_end = int(0.7 * n)\n",
        "    val_end = int(0.85 * n)\n",
        "\n",
        "    train_pairs.extend(pairs[:train_end])\n",
        "    val_pairs.extend(pairs[train_end:val_end])\n",
        "    test_pairs.extend(pairs[val_end:])\n",
        "\n",
        "print(f\"\\nSplit results (maintaining 70/15/15 ratio):\")\n",
        "print(f\"Training samples: {len(train_pairs)} ({len(train_pairs)/total_samples*100:.1f}%)\")\n",
        "print(f\"Validation samples: {len(val_pairs)} ({len(val_pairs)/total_samples*100:.1f}%)\")\n",
        "print(f\"Test samples: {len(test_pairs)} ({len(test_pairs)/total_samples*100:.1f}%)\")\n",
        "\n",
        "# Preprocessing functions\n",
        "def preprocess_image_array(img, target_size=TARGET_SIZE, grayscale=False):\n",
        "    if img is None:\n",
        "        return None\n",
        "\n",
        "    if grayscale and len(img.shape) == 3:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    h, w = img.shape[:2]\n",
        "\n",
        "    # Smart padding to maintain aspect ratio\n",
        "    if h > w:\n",
        "        pad_top = 0\n",
        "        pad_bottom = 0\n",
        "        pad_left = (h - w) // 2\n",
        "        pad_right = h - w - pad_left\n",
        "    else:\n",
        "        pad_left = 0\n",
        "        pad_right = 0\n",
        "        pad_top = (w - h) // 2\n",
        "        pad_bottom = w - h - pad_top\n",
        "\n",
        "    # Pad to square\n",
        "    if len(img.shape) == 3:\n",
        "        padded = cv2.copyMakeBorder(img, pad_top, pad_bottom, pad_left, pad_right,\n",
        "                                   cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "    else:\n",
        "        padded = cv2.copyMakeBorder(img, pad_top, pad_bottom, pad_left, pad_right,\n",
        "                                   cv2.BORDER_CONSTANT, value=0)\n",
        "\n",
        "    # Resize\n",
        "    resized = cv2.resize(padded, (target_size, target_size), interpolation=cv2.INTER_LANCZOS4)\n",
        "\n",
        "    # Normalize to [0, 1]\n",
        "    return resized.astype(np.float32) / 255.0\n",
        "\n",
        "def load_single_pair_optimized(pair_info):\n",
        "    try:\n",
        "        # Load damaged image\n",
        "        damaged_img = cv2.imread(pair_info['damaged'], cv2.IMREAD_COLOR)\n",
        "        if damaged_img is None:\n",
        "            return None, None, f\"Failed to load damaged: {pair_info['damaged']}\"\n",
        "\n",
        "        # Load mask\n",
        "        mask_img = cv2.imread(pair_info['mask'], cv2.IMREAD_GRAYSCALE)\n",
        "        if mask_img is None:\n",
        "            return None, None, f\"Failed to load mask: {pair_info['mask']}\"\n",
        "\n",
        "        # Preprocess\n",
        "        damaged_processed = preprocess_image_array(damaged_img, TARGET_SIZE)\n",
        "        mask_processed = preprocess_image_array(mask_img, TARGET_SIZE, grayscale=True)\n",
        "\n",
        "        if damaged_processed is None or mask_processed is None:\n",
        "            return None, None, \"Preprocessing failed\"\n",
        "\n",
        "        # Add channel dimension to mask\n",
        "        mask_processed = mask_processed[..., np.newaxis]\n",
        "\n",
        "        return damaged_processed, mask_processed, None\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, None, f\"Error: {str(e)}\"\n",
        "\n",
        "\n",
        "# Optimized revision of batch loading function\n",
        "def batch_load_optimized(pairs_list, desc=\"Loading\"):\n",
        "    # Batch loading\n",
        "    if len(pairs_list) == 0:\n",
        "        return np.array([]), np.array([])\n",
        "\n",
        "    print(f\"{desc} {len(pairs_list)} samples...\")\n",
        "\n",
        "    images = []\n",
        "    masks = []\n",
        "    errors = []\n",
        "\n",
        "    # Process in smaller batches to manage memory\n",
        "    batch_size = min(100, len(pairs_list))\n",
        "\n",
        "    for i in tqdm(range(0, len(pairs_list), batch_size), desc=desc):\n",
        "        batch_pairs = pairs_list[i:i+batch_size]\n",
        "\n",
        "        # Parallel processing for current batch\n",
        "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "            futures = [executor.submit(load_single_pair_optimized, pair) for pair in batch_pairs]\n",
        "\n",
        "            for future in futures:\n",
        "                try:\n",
        "                    img, mask, error = future.result()\n",
        "                    if error:\n",
        "                        errors.append(error)\n",
        "                    else:\n",
        "                        images.append(img)\n",
        "                        masks.append(mask)\n",
        "                except Exception as e:\n",
        "                    errors.append(f\"Batch processing error: {str(e)}\")\n",
        "\n",
        "    if errors:\n",
        "        print(f\"{len(errors)} errors encountered\")\n",
        "        if len(errors) <= 5:\n",
        "            for error in errors:\n",
        "                print(f\"   {error}\")\n",
        "\n",
        "    print(f\"Successfully loaded {len(images)} pairs\")\n",
        "\n",
        "    if len(images) == 0:\n",
        "        return np.array([]), np.array([])\n",
        "\n",
        "    return np.array(images, dtype=np.float32), np.array(masks, dtype=np.float32)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Load training data\n",
        "train_images, train_masks = batch_load_optimized(train_pairs, \"Training\")\n",
        "gc.collect()\n",
        "\n",
        "# Load validation data\n",
        "val_images, val_masks = batch_load_optimized(val_pairs, \"Validation\")\n",
        "gc.collect()\n",
        "\n",
        "# Load test data\n",
        "test_images, test_masks = batch_load_optimized(test_pairs, \"Test\")\n",
        "gc.collect()\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "# Final dataset info\n",
        "total_loaded = len(train_images) + len(val_images) + len(test_images)\n",
        "current_memory = psutil.virtual_memory()\n",
        "\n",
        "# Data quality validation\n",
        "if len(train_images) > 0:\n",
        "    if np.any(np.isnan(train_images)) or np.any(np.isinf(train_images)):\n",
        "        raise ValueError(\"NaN or infinite values in training images\")\n",
        "    if np.any(np.isnan(train_masks)) or np.any(np.isinf(train_masks)):\n",
        "        raise ValueError(\"NaN or infinite values in training masks\")"
      ],
      "metadata": {
        "id": "7b1a4Bh2vTGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define attention gate and encoder decoder blocks\n",
        "\n",
        "def attention_gate(x, g, inter_channels):\n",
        "    # Linear transformations\n",
        "    theta_x = layers.Conv2D(inter_channels, 1, padding='same')(x)\n",
        "    phi_g = layers.Conv2D(inter_channels, 1, padding='same')(g)\n",
        "\n",
        "    # Upsample gating signal to match x dimensions\n",
        "    phi_g = layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(phi_g)\n",
        "\n",
        "    # If dimensions still don't match, use resize layer\n",
        "    phi_g = layers.Resizing(theta_x.shape[1], theta_x.shape[2])(phi_g)\n",
        "\n",
        "    # Add and apply activation\n",
        "    add_xg = layers.Add()([theta_x, phi_g])\n",
        "    add_xg = layers.Activation('relu')(add_xg)\n",
        "\n",
        "    # Generate attention coefficients\n",
        "    psi = layers.Conv2D(1, 1, padding='same')(add_xg)\n",
        "    psi = layers.Activation('sigmoid')(psi)\n",
        "\n",
        "    # Apply attention weights\n",
        "    x_att = layers.Multiply()([x, psi])\n",
        "\n",
        "    return x_att\n",
        "\n",
        "def encoder_block(inputs, num_filters):\n",
        "    # Encoder block: applies two convolutional layers followed by max pooling.\n",
        "    x = layers.Conv2D(num_filters, 3, padding='same')(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.Conv2D(num_filters, 3, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    skip_features = x\n",
        "    x = layers.MaxPool2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    return x, skip_features\n",
        "\n",
        "def decoder_block(inputs, skip_features, num_filters):\n",
        "    # Upsample\n",
        "    x = layers.Conv2DTranspose(num_filters, (2, 2), strides=2, padding='same')(inputs)\n",
        "\n",
        "    # Apply attention gate\n",
        "    skip_features = attention_gate(skip_features, inputs, num_filters // 2)\n",
        "\n",
        "    # Concatenate skip connection\n",
        "    x = layers.Concatenate()([x, skip_features])\n",
        "\n",
        "    # Convolutional layers\n",
        "    x = layers.Conv2D(num_filters, 3, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.Conv2D(num_filters, 3, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def build_attention_unet(input_shape=(256, 256, 3)):\n",
        "    # Build the Unet\n",
        "    inputs = layers.Input(input_shape)\n",
        "\n",
        "    # Encoder path\n",
        "    e1, skip1 = encoder_block(inputs, 64)     # 128x128\n",
        "    e2, skip2 = encoder_block(e1, 128)        # 64x64\n",
        "    e3, skip3 = encoder_block(e2, 256)        # 32x32\n",
        "    e4, skip4 = encoder_block(e3, 512)        # 16x16\n",
        "\n",
        "    # Bottleneck\n",
        "    b = layers.Conv2D(1024, 3, padding='same')(e4)\n",
        "    b = layers.BatchNormalization()(b)\n",
        "    b = layers.Activation('relu')(b)\n",
        "    b = layers.Conv2D(1024, 3, padding='same')(b)\n",
        "    b = layers.BatchNormalization()(b)\n",
        "    b = layers.Activation('relu')(b)\n",
        "\n",
        "    # Decoder path with attention gates\n",
        "    d1 = decoder_block(b, skip4, 512)     # 32x32\n",
        "    d2 = decoder_block(d1, skip3, 256)    # 64x64\n",
        "    d3 = decoder_block(d2, skip2, 128)    # 128x128\n",
        "    d4 = decoder_block(d3, skip1, 64)     # 256x256\n",
        "\n",
        "    # Final output layer\n",
        "    outputs = layers.Conv2D(1, 1, activation='sigmoid', padding='same')(d4)\n",
        "\n",
        "    return tf.keras.Model(inputs, outputs, name='AttentionUNet')\n",
        "\n",
        "def focal_tversky_loss(y_true, y_pred, alpha=0.7, gamma=0.75):\n",
        "    y_true = tf.cast(y_true > 0.5, tf.float32)\n",
        "\n",
        "    # Calculate Tversky components\n",
        "    tp = tf.reduce_sum(y_true * y_pred)\n",
        "    fn = tf.reduce_sum(y_true * (1 - y_pred))\n",
        "    fp = tf.reduce_sum((1 - y_true) * y_pred)\n",
        "\n",
        "    # Tversky index\n",
        "    tversky = (tp + 1e-6) / (tp + alpha * fn + (1 - alpha) * fp + 1e-6)\n",
        "\n",
        "    # Focal Tversky Loss\n",
        "    return tf.pow(1 - tversky, gamma)\n",
        "\n",
        "# Build and compile the model\n",
        "model = build_attention_unet(input_shape=(256, 256, 3))\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss=focal_tversky_loss,\n",
        "    metrics=[\n",
        "        'accuracy',\n",
        "        tf.keras.metrics.MeanIoU(num_classes=2),\n",
        "        tf.keras.metrics.Precision(),\n",
        "        tf.keras.metrics.Recall()\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Display model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "QKQKodiVyg9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks for training\n",
        "# Stop training if validation loss doesnâ€™t improve for 5 epochs, restoring best weights, save the best model based on validation lose, and reduce learning rate in case it plateaus\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        patience=5,\n",
        "        monitor='val_loss',\n",
        "        restore_best_weights=True\n",
        "    ),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        'best_model.h5',\n",
        "        save_best_only=True,\n",
        "        monitor='val_loss'\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        factor=0.1,\n",
        "        patience=3,\n",
        "        verbose=1\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "jtCLJi-x3nhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training configuration\n",
        "EPOCHS = 40\n",
        "BATCH_SIZE = 8  # You can adjust this based on your GPU memory\n",
        "\n",
        "# Verify data shapes\n",
        "print(\"\\nFinal data shapes verification:\")\n",
        "print(f\"Train images: {train_images.shape}, masks: {train_masks.shape}\")\n",
        "print(f\"Val images: {val_images.shape}, masks: {val_masks.shape}\")\n",
        "\n",
        "# Training loop\n",
        "history = model.fit(\n",
        "    x=train_images,\n",
        "    y=train_masks,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(val_images, val_masks),\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "YT3PT_Th3j8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next couple of cells are just to deploy the model to HuggingFace\n",
        "\n",
        "!pip install huggingface_hub -q"
      ],
      "metadata": {
        "id": "ZuAwD6wy2kYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "DUiTtUHt3FCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import create_repo\n",
        "\n",
        "repo_id = \"maximiliannl/artwork-damage-detector\"\n",
        "create_repo(repo_id, repo_type=\"model\", exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "RylpM_Yq5A2z",
        "outputId": "ad5e1f68-f1d6-4ef6-df87-430eb82d19b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RepoUrl('https://huggingface.co/maximiliannl/artwork-damage-detector', endpoint='https://huggingface.co', repo_type='model', repo_id='maximiliannl/artwork-damage-detector')"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"artwork_damage_detector.keras\")"
      ],
      "metadata": {
        "id": "8BU_VECv3S4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "repo_id = \"maximiliannl/artwork-damage-detector\"\n",
        "\n",
        "api = HfApi()\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"artwork_damage_detector.keras\",\n",
        "    path_in_repo=\"model.keras\",\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\",\n",
        ")"
      ],
      "metadata": {
        "id": "W_X65JPX4nmz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}