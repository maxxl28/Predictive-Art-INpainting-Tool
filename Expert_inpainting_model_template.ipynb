{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# README:\n",
        "Train a Stable Diffusion inpainting pipeline to automatically restore damaged artwork. Use encoder-decoder diffusion architecture for high-quality image completion, and specialized loss functions to focus on authentic historical style reconstruction rather than generic inpainting. Use memory-optimized training with gradient accumulation and mixed precision for efficient fine-tuning."
      ],
      "metadata": {
        "id": "ILlLQAQV-r-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q diffusers transformers accelerate datasets xformers ftfy bitsandbytes huggingface_hub torchvision tqdm wandb"
      ],
      "metadata": {
        "id": "WWvHLnuqRbvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from PIL import Image, ImageOps\n",
        "from tqdm.auto import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from datasets import Dataset as HFDataset\n",
        "import random\n",
        "import logging\n",
        "import json\n",
        "from datetime import datetime\n",
        "from diffusers import (\n",
        "    StableDiffusionInpaintPipeline,\n",
        "    DDPMScheduler,\n",
        "    UNet2DConditionModel,\n",
        "    AutoencoderKL\n",
        ")\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim"
      ],
      "metadata": {
        "id": "v0o-xhinSBF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "0PulgiW0Rd1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xDDjpHDfSBoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a class to hold all configuration parameters\n",
        "\n",
        "class TrainingConfig:\n",
        "    def __init__(self):\n",
        "        # Paths\n",
        "        self.damaged_dir = \"/content/drive/My Drive/complete_dataset_25k_plus/damaged\"\n",
        "        self.mask_dir = \"/content/drive/My Drive/complete_dataset_25k_plus/masks\"\n",
        "        self.target_dir = \"/content/drive/My Drive/complete_dataset_25k_plus/Data Categories\"\n",
        "        self.export_dir = \"/content/inpaint_dataset\"\n",
        "        self.output_dir = \"/content/drive/My Drive/sd-inpainting-finetuned\"\n",
        "\n",
        "        # Training parameters\n",
        "        self.model_id = \"runwayml/stable-diffusion-inpainting\"\n",
        "        self.train_batch_size = 1\n",
        "        self.gradient_accumulation_steps = 8\n",
        "        self.learning_rate = 5e-6\n",
        "        self.num_train_epochs = 10\n",
        "        self.save_model_epochs = 2\n",
        "        self.max_grad_norm = 1.0\n",
        "        self.warmup_steps = 100\n",
        "        self.logging_steps = 10\n",
        "        self.validation_steps = 50\n",
        "\n",
        "        # Image parameters\n",
        "        self.resolution = 512\n",
        "        self.valid_extensions = {'.jpg', '.jpeg', '.png'}\n",
        "\n",
        "        # Device and memory settings\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.mixed_precision = True\n",
        "        self.enable_xformers = True\n",
        "        self.gradient_checkpointing = True\n",
        "\n",
        "    def validate_paths(self):\n",
        "        \"\"\"Validate all required paths exist\"\"\"\n",
        "        paths_to_check = [self.damaged_dir, self.mask_dir, self.target_dir]\n",
        "        for path in paths_to_check:\n",
        "            if not os.path.exists(path):\n",
        "                raise FileNotFoundError(f\"Required directory not found: {path}\")\n",
        "\n",
        "        # Create output directories\n",
        "        os.makedirs(self.export_dir, exist_ok=True)\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "config = TrainingConfig()\n",
        "config.validate_paths()"
      ],
      "metadata": {
        "id": "dr5sCgFjSDOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finds files with valid extensions and strip suffixes from name\n",
        "\n",
        "def get_files_map_recursive(directory, suffixes, valid_extensions):\n",
        "    files_map = {}\n",
        "    if not os.path.exists(directory):\n",
        "        logger.warning(f\"Directory does not exist: {directory}\")\n",
        "        return files_map\n",
        "\n",
        "    try:\n",
        "        pattern = os.path.join(directory, '**', '*.*')\n",
        "        all_files = glob(pattern, recursive=True)\n",
        "\n",
        "        for filepath in all_files:\n",
        "            try:\n",
        "                ext = os.path.splitext(filepath)[1].lower()\n",
        "                if ext not in valid_extensions:\n",
        "                    continue\n",
        "\n",
        "                fname = os.path.basename(filepath)\n",
        "                root, _ = os.path.splitext(fname)\n",
        "\n",
        "                for suf in suffixes:\n",
        "                    if suf == '':\n",
        "                        if not (root.endswith('_damaged') or root.endswith('_mask')):\n",
        "                            base = root\n",
        "                            files_map[base] = filepath\n",
        "                            break\n",
        "                    else:\n",
        "                        if root.endswith(suf):\n",
        "                            base = root[:-len(suf)]\n",
        "                            files_map[base] = filepath\n",
        "                            break\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error processing file {filepath}: {e}\")\n",
        "                continue\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error scanning directory {directory}: {e}\")\n",
        "\n",
        "    return files_map\n"
      ],
      "metadata": {
        "id": "VWzvC9hZU5EU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads and processes an image, pad to square and resize\n",
        "def pad_and_resize_safe(image_path, target_size=(512, 512)):\n",
        "    \"\"\"Safe image padding and resizing with error handling\"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(image_path):\n",
        "            raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
        "\n",
        "        img = Image.open(image_path).convert(\"RGB\")\n",
        "        width, height = img.size\n",
        "\n",
        "        if width == 0 or height == 0:\n",
        "            raise ValueError(f\"Invalid image dimensions: {width}x{height}\")\n",
        "\n",
        "        max_side = max(width, height)\n",
        "        delta_w = max_side - width\n",
        "        delta_h = max_side - height\n",
        "        padding = (delta_w//2, delta_h//2, delta_w - delta_w//2, delta_h - delta_h//2)\n",
        "        padded = ImageOps.expand(img, padding, fill=(0, 0, 0))\n",
        "        resized = padded.resize(target_size, Image.LANCZOS)\n",
        "        return resized\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing image {image_path}: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "BKCRt-QpU7T-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to find triplets of files: a damaged image, its mask, and the clean target image\n",
        "\n",
        "def find_triplets_recursive_safe(category, damaged_dir, mask_dir, target_dir, valid_extensions):\n",
        "    \"\"\"Find matching triplets with comprehensive error handling\"\"\"\n",
        "    logger.info(f\"Processing category: {category}\")\n",
        "\n",
        "    cat_damaged = os.path.join(damaged_dir, category)\n",
        "    cat_mask = os.path.join(mask_dir, category)\n",
        "    cat_target = os.path.join(target_dir, category)\n",
        "\n",
        "    # Check if category directories exist\n",
        "    missing_dirs = []\n",
        "    for name, path in [(\"damaged\", cat_damaged), (\"mask\", cat_mask), (\"target\", cat_target)]:\n",
        "        if not os.path.exists(path):\n",
        "            missing_dirs.append(f\"{name}: {path}\")\n",
        "\n",
        "    if missing_dirs:\n",
        "        logger.warning(f\"Missing directories for category {category}: {missing_dirs}\")\n",
        "        return []\n",
        "\n",
        "    damaged_map = get_files_map_recursive(cat_damaged, ['_damaged'], valid_extensions)\n",
        "    mask_map = get_files_map_recursive(cat_mask, ['_mask'], valid_extensions)\n",
        "    target_map = get_files_map_recursive(cat_target, [''], valid_extensions)\n",
        "\n",
        "    logger.info(f\"Found - Damaged: {len(damaged_map)}, Masks: {len(mask_map)}, Targets: {len(target_map)}\")\n",
        "\n",
        "    common_keys = set(damaged_map.keys()) & set(mask_map.keys()) & set(target_map.keys())\n",
        "    logger.info(f\"Complete triplets: {len(common_keys)}\")\n",
        "\n",
        "    triplets = []\n",
        "    for base in common_keys:\n",
        "        # Verify all files exist and are readable\n",
        "        files_to_check = [\n",
        "            ('damaged', damaged_map[base]),\n",
        "            ('mask', mask_map[base]),\n",
        "            ('target', target_map[base])\n",
        "        ]\n",
        "\n",
        "        valid_triplet = True\n",
        "        for file_type, file_path in files_to_check:\n",
        "            if not os.path.exists(file_path):\n",
        "                logger.warning(f\"Missing {file_type} file: {file_path}\")\n",
        "                valid_triplet = False\n",
        "                break\n",
        "            try:\n",
        "                # Quick check if file is a valid image\n",
        "                with Image.open(file_path) as img:\n",
        "                    pass\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Invalid {file_type} image {file_path}: {e}\")\n",
        "                valid_triplet = False\n",
        "                break\n",
        "\n",
        "        if valid_triplet:\n",
        "            triplets.append({\n",
        "                'name': base,\n",
        "                'category': category,\n",
        "                'damaged': damaged_map[base],\n",
        "                'mask': mask_map[base],\n",
        "                'target': target_map[base]\n",
        "            })\n",
        "\n",
        "    logger.info(f\"Valid triplets after verification: {len(triplets)}\")\n",
        "    return triplets\n"
      ],
      "metadata": {
        "id": "K1kpgJx3U9t1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set category\n",
        "category = \"Medieval\"\n",
        "logger.info(f\"Starting processing for category: {category}\")\n",
        "\n",
        "triplets = find_triplets_recursive_safe(\n",
        "    category, config.damaged_dir, config.mask_dir,\n",
        "    config.target_dir, config.valid_extensions\n",
        ")\n",
        "\n",
        "if len(triplets) == 0:\n",
        "    raise ValueError(f\"No valid triplets found for category {category}\")\n",
        "\n",
        "logger.info(f\"Found {len(triplets)} valid triplets\")"
      ],
      "metadata": {
        "id": "EkyWLtI0VCHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the dataset\n",
        "\n",
        "def prepare_dataset_safe(triplets, export_dir, resolution=512):\n",
        "    \"\"\"Safely prepare dataset with error handling and validation\"\"\"\n",
        "    successful_conversions = 0\n",
        "    failed_conversions = 0\n",
        "\n",
        "    for i, triplet in enumerate(tqdm(triplets, desc=\"Processing images\")):\n",
        "        try:\n",
        "            base = f\"img_{i:04d}\"\n",
        "\n",
        "            # Process each image with error checking\n",
        "            damaged_img = pad_and_resize_safe(triplet[\"damaged\"], (resolution, resolution))\n",
        "            mask_img = pad_and_resize_safe(triplet[\"mask\"], (resolution, resolution))\n",
        "            target_img = pad_and_resize_safe(triplet[\"target\"], (resolution, resolution))\n",
        "\n",
        "            if damaged_img is None or mask_img is None or target_img is None:\n",
        "                logger.warning(f\"Failed to process triplet {triplet['name']}\")\n",
        "                failed_conversions += 1\n",
        "                continue\n",
        "\n",
        "            # Save images\n",
        "            damaged_img.save(os.path.join(export_dir, f\"{base}.png\"))\n",
        "            mask_img.save(os.path.join(export_dir, f\"{base}_mask.png\"))\n",
        "            target_img.save(os.path.join(export_dir, f\"{base}_target.png\"))\n",
        "\n",
        "            successful_conversions += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing triplet {triplet['name']}: {e}\")\n",
        "            failed_conversions += 1\n",
        "            continue\n",
        "\n",
        "    logger.info(f\"Dataset preparation complete: {successful_conversions} successful, {failed_conversions} failed\")\n",
        "    return successful_conversions"
      ],
      "metadata": {
        "id": "UmNQ90weVEPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "successful_count = prepare_dataset_safe(triplets, config.export_dir, config.resolution)"
      ],
      "metadata": {
        "id": "WhaEtdMrVHKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompts(category):\n",
        "    return (\n",
        "        f\"You are an expert art restorer. \"\n",
        "        f\"Carefully restore and complete this {category} era artwork, \"\n",
        "        f\"preserving its original composition, brushwork, and textures. \"\n",
        "        f\"Emphasize historical accuracy, authentic color palettes, and fine detail \"\n",
        "        f\"true to the {category} period.\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "8HuDXx31VJ4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare dataset for training by combining images, masks, and prompts\n",
        "\n",
        "examples = []\n",
        "processed_count = 0\n",
        "for i in range(len(triplets)):\n",
        "    base = f\"img_{i:04d}\"\n",
        "    image_path = os.path.join(config.export_dir, f\"{base}.png\")\n",
        "    mask_path = os.path.join(config.export_dir, f\"{base}_mask.png\")\n",
        "\n",
        "    if os.path.exists(image_path) and os.path.exists(mask_path):\n",
        "        prompt = generate_prompts(triplets[i]['category'])\n",
        "        examples.append({\n",
        "            \"image\": image_path,\n",
        "            \"mask\": mask_path,\n",
        "            \"prompt\": prompt\n",
        "        })\n",
        "        processed_count += 1\n",
        "\n",
        "logger.info(f\"Created {processed_count} dataset examples\")\n",
        "\n",
        "if len(examples) < 10:\n",
        "    raise ValueError(f\"Too few examples ({len(examples)}) for training\")\n",
        "\n",
        "# Create and split dataset\n",
        "random.shuffle(examples)\n",
        "dataset = HFDataset.from_list(examples)\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "logger.info(f\"Dataset split - Train: {len(dataset['train'])}, Test: {len(dataset['test'])}\")"
      ],
      "metadata": {
        "id": "ijc1XaJZVcIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch dataset: convert the images and masks into tensors and tokenize the prompt\n",
        "\n",
        "class SafeInpaintingDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer, size=512):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.size = size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            item = self.dataset[idx]\n",
        "\n",
        "            # Load and validate images\n",
        "            if not os.path.exists(item[\"image\"]):\n",
        "                raise FileNotFoundError(f\"Image not found: {item['image']}\")\n",
        "            if not os.path.exists(item[\"mask\"]):\n",
        "                raise FileNotFoundError(f\"Mask not found: {item['mask']}\")\n",
        "\n",
        "            image = Image.open(item[\"image\"]).convert(\"RGB\")\n",
        "            mask = Image.open(item[\"mask\"]).convert(\"L\")\n",
        "\n",
        "            # Validate image dimensions\n",
        "            if image.size != (self.size, self.size) or mask.size != (self.size, self.size):\n",
        "                image = image.resize((self.size, self.size), Image.LANCZOS)\n",
        "                mask = mask.resize((self.size, self.size), Image.LANCZOS)\n",
        "\n",
        "            # Convert to tensor with proper normalization\n",
        "            image = torch.from_numpy(np.array(image)).float() / 127.5 - 1.0\n",
        "            mask = torch.from_numpy(np.array(mask)).float() / 255.0\n",
        "\n",
        "            mask = (mask > 0.5).float()\n",
        "\n",
        "            # Tokenize prompt with error handling\n",
        "            prompt = item.get(\"prompt\", \"\")\n",
        "            text_input = self.tokenizer(\n",
        "                prompt,\n",
        "                padding=\"max_length\",\n",
        "                max_length=self.tokenizer.model_max_length,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                \"pixel_values\": image.permute(2, 0, 1),\n",
        "                \"mask_values\": mask.unsqueeze(0),\n",
        "                \"input_ids\": text_input.input_ids.squeeze(),\n",
        "                \"attention_mask\": text_input.attention_mask.squeeze()\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading dataset item {idx}: {e}\")"
      ],
      "metadata": {
        "id": "TCejas5fVgVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Safely load the core models for inpainting pipeline\n",
        "\n",
        "def load_models_safe(config):\n",
        "    try:\n",
        "        logger.info(\"Loading tokenizer and text encoder...\")\n",
        "        tokenizer = CLIPTokenizer.from_pretrained(\n",
        "            config.model_id,\n",
        "            subfolder=\"tokenizer\",\n",
        "            use_fast=False  # More stable for fine-tuning\n",
        "        )\n",
        "        text_encoder = CLIPTextModel.from_pretrained(\n",
        "            config.model_id,\n",
        "            subfolder=\"text_encoder\"\n",
        "        ).to(config.device)\n",
        "\n",
        "        logger.info(\"Loading VAE...\")\n",
        "        vae = AutoencoderKL.from_pretrained(\n",
        "            config.model_id,\n",
        "            subfolder=\"vae\",\n",
        "            low_cpu_mem_usage=True\n",
        "        ).to(config.device)\n",
        "\n",
        "        logger.info(\"Loading UNet...\")\n",
        "        unet = UNet2DConditionModel.from_pretrained(\n",
        "            config.model_id,\n",
        "            subfolder=\"unet\",\n",
        "            low_cpu_mem_usage=True,\n",
        "            torch_dtype=torch.float16 if config.mixed_precision else torch.float32\n",
        "        )\n",
        "\n",
        "        # Enable memory-saving features\n",
        "        if config.gradient_checkpointing:\n",
        "            unet.enable_gradient_checkpointing()\n",
        "        if config.enable_xformers:\n",
        "            unet.set_use_memory_efficient_attention_xformers(True)\n",
        "\n",
        "        unet.to(config.device)\n",
        "\n",
        "        # Freeze models that shouldn't be trained\n",
        "        vae.requires_grad_(False)\n",
        "        text_encoder.requires_grad_(False)\n",
        "\n",
        "        logger.info(\"Model loading complete\")\n",
        "        return tokenizer, text_encoder, vae, unet\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Model loading failed: {e}\")\n",
        "        raise\n",
        "\n",
        "tokenizer, text_encoder, vae, unet = load_models_safe(config)"
      ],
      "metadata": {
        "id": "_yE1T1yvqYKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset = SafeInpaintingDataset(dataset[\"train\"], tokenizer, config.resolution)\n",
        "val_dataset = SafeInpaintingDataset(dataset[\"test\"], tokenizer, config.resolution)\n",
        "\n",
        "# Memory-optimized data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config.train_batch_size,\n",
        "    shuffle=True,\n",
        "    pin_memory=True,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=1,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "UJdfD6sJqjmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer (only train UNet parameters)\n",
        "optimizer = AdamW(\n",
        "    unet.parameters(),\n",
        "    lr=config.learning_rate,\n",
        "    weight_decay=1e-6\n",
        ")\n",
        "\n",
        "# Learning rate scheduler\n",
        "lr_scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=config.warmup_steps,\n",
        "    num_training_steps=len(train_loader) * config.num_train_epochs\n",
        ")\n",
        "\n",
        "# Noise scheduler\n",
        "noise_scheduler = DDPMScheduler.from_pretrained(config.model_id, subfolder=\"scheduler\")\n",
        "\n",
        "# Loss function\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Mixed precision scaler\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=config.mixed_precision)"
      ],
      "metadata": {
        "id": "UBv0_yMaqxkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trains the UNet for a single epoch on difussion pods\n",
        "\n",
        "def train_one_epoch(unet, vae, text_encoder, train_loader, optimizer,\n",
        "                   noise_scheduler, scaler, epoch, config):\n",
        "    unet.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch + 1}\")):\n",
        "        # Move batch to device\n",
        "        pixel_values = batch[\"pixel_values\"].to(config.device)\n",
        "        mask_values = batch[\"mask_values\"].to(config.device)\n",
        "        input_ids = batch[\"input_ids\"].to(config.device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(config.device)\n",
        "\n",
        "        # Get text embeddings\n",
        "        with torch.no_grad():\n",
        "            text_embeddings = text_encoder(\n",
        "                input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )[0]\n",
        "\n",
        "        # Prepare masked images\n",
        "        masked_images = pixel_values * (mask_values < 0.5)\n",
        "\n",
        "        # Convert images to latent space\n",
        "        with torch.no_grad():\n",
        "            latents = vae.encode(pixel_values).latent_dist.sample()\n",
        "            latents = latents * 0.18215\n",
        "            masked_latents = vae.encode(masked_images).latent_dist.sample()\n",
        "            masked_latents = masked_latents * 0.18215\n",
        "\n",
        "        # Sample noise and add to latents\n",
        "        noise = torch.randn_like(latents)\n",
        "        timesteps = torch.randint(\n",
        "            0, noise_scheduler.config.num_train_timesteps,\n",
        "            (latents.shape[0],), device=config.device\n",
        "        ).long()\n",
        "\n",
        "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "        # Forward pass with mixed precision\n",
        "        with torch.amp.autocast(device_type='cuda', enabled=config.mixed_precision):\n",
        "            mask_resized = F.interpolate(mask_values, size=latents.shape[2:], mode='nearest')\n",
        "\n",
        "            # Concatenate inputs\n",
        "            model_input = torch.cat([\n",
        "                noisy_latents,          # 4 channels\n",
        "                masked_latents,         # 4 channels\n",
        "                mask_resized            # 1 channel\n",
        "            ], dim=1)\n",
        "\n",
        "            noise_pred = unet(model_input, timesteps, text_embeddings).sample\n",
        "\n",
        "            # Calculate loss only on masked regions\n",
        "            mask = mask_resized.expand(-1, 4, -1, -1)\n",
        "            loss = loss_fn(noise_pred[mask > 0], noise[mask > 0])\n",
        "            loss = loss / config.gradient_accumulation_steps\n",
        "\n",
        "        # Backward pass with gradient scaling\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Gradient accumulation\n",
        "        if (step + 1) % config.gradient_accumulation_steps == 0:\n",
        "            if config.mixed_precision:\n",
        "                # Mixed precision path with proper FP16 handling\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(unet.parameters(), config.max_grad_norm)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                # Non-mixed precision path\n",
        "                torch.nn.utils.clip_grad_norm_(unet.parameters(), config.max_grad_norm)\n",
        "                optimizer.step()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Logging\n",
        "        if step % config.logging_steps == 0:\n",
        "            avg_loss = total_loss / (step + 1)\n",
        "            lr = optimizer.param_groups[0][\"lr\"]\n",
        "            logger.info(f\"Step {step}: Loss {avg_loss:.4f}, LR {lr:.2e}\")\n",
        "\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def validate(unet, vae, text_encoder, val_loader, noise_scheduler, epoch, config):\n",
        "    unet.eval()\n",
        "    val_loss = 0\n",
        "    psnr_values = []\n",
        "    ssim_values = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
        "            pixel_values = batch[\"pixel_values\"].to(config.device)\n",
        "            mask_values = batch[\"mask_values\"].to(config.device)\n",
        "            input_ids = batch[\"input_ids\"].to(config.device)\n",
        "\n",
        "            # Get text embeddings\n",
        "            text_embeddings = text_encoder(input_ids)[0]\n",
        "\n",
        "            # Prepare masked images\n",
        "            masked_images = pixel_values * (mask_values < 0.5)\n",
        "\n",
        "            # Convert to latent space\n",
        "            latents = vae.encode(pixel_values).latent_dist.sample() * 0.18215\n",
        "            masked_latents = vae.encode(masked_images).latent_dist.sample() * 0.18215\n",
        "\n",
        "            # Sample noise\n",
        "            noise = torch.randn_like(latents)\n",
        "            timesteps = torch.zeros((1,), device=config.device).long()\n",
        "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "            # Resize mask to match latent dimensions\n",
        "            mask_resized = F.interpolate(mask_values, size=latents.shape[2:], mode='nearest')\n",
        "\n",
        "            # Predict noise\n",
        "            model_input = torch.cat([\n",
        "                noisy_latents,\n",
        "                masked_latents,\n",
        "                mask_resized\n",
        "            ], dim=1)\n",
        "            noise_pred = unet(model_input, timesteps, text_embeddings).sample\n",
        "\n",
        "            # Calculate loss\n",
        "            mask = mask_resized.expand(-1, 4, -1, -1)\n",
        "            loss = loss_fn(noise_pred[mask > 0], noise[mask > 0])\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Generate reconstruction for metrics\n",
        "            pred_latents = (noisy_latents - noise_pred) / noise_scheduler.init_noise_sigma\n",
        "            pred_images = vae.decode(pred_latents / 0.18215).sample\n",
        "\n",
        "            # Calculate metrics\n",
        "            pred_np = pred_images.squeeze().cpu().numpy().transpose(1, 2, 0)\n",
        "            target_np = pixel_values.squeeze().cpu().numpy().transpose(1, 2, 0)\n",
        "\n",
        "            # Normalize for metrics\n",
        "            pred_np = (pred_np + 1) / 2  # [-1,1] -> [0,1]\n",
        "            target_np = (target_np + 1) / 2\n",
        "\n",
        "            # Calculate PSNR and SSIM on unmasked regions\n",
        "            mask_np = mask_values.squeeze().cpu().numpy() < 0.5\n",
        "            if mask_np.any():\n",
        "                psnr_val = psnr(target_np, pred_np, data_range=1.0)\n",
        "                ssim_val = ssim(target_np, pred_np, channel_axis=-1, data_range=1.0)\n",
        "                psnr_values.append(psnr_val)\n",
        "                ssim_values.append(ssim_val)\n",
        "\n",
        "    avg_loss = val_loss / len(val_loader)\n",
        "    avg_psnr = np.mean(psnr_values) if psnr_values else 0\n",
        "    avg_ssim = np.mean(ssim_values) if ssim_values else 0\n",
        "\n",
        "    logger.info(f\"Validation - Loss: {avg_loss:.4f}, PSNR: {avg_psnr:.2f}, SSIM: {avg_ssim:.4f}\")\n",
        "\n",
        "    return avg_loss, avg_psnr, avg_ssim"
      ],
      "metadata": {
        "id": "olXt-Uhdqy-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop with checkpointing and improved error handling\n",
        "best_psnr = 0\n",
        "\n",
        "# Ensure UNet parameters are in the right dtype\n",
        "if config.mixed_precision:\n",
        "    # This might throw some errors\n",
        "    unet = unet.float()\n",
        "\n",
        "# Create scaler with proper settings for FP16 handling\n",
        "if config.mixed_precision:\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
        "else:\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=False)\n",
        "\n",
        "for epoch in range(config.num_train_epochs):\n",
        "    try:\n",
        "        # Train\n",
        "        train_loss = train_one_epoch(\n",
        "            unet, vae, text_encoder, train_loader, optimizer,\n",
        "            noise_scheduler, scaler, epoch, config\n",
        "        )\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_psnr, val_ssim = validate(\n",
        "            unet, vae, text_encoder, val_loader,\n",
        "            noise_scheduler, epoch, config\n",
        "        )\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (epoch + 1) % config.save_model_epochs == 0 or val_psnr > best_psnr:\n",
        "            checkpoint_path = os.path.join(config.output_dir, f\"checkpoint-{epoch}\")\n",
        "            os.makedirs(checkpoint_path, exist_ok=True)\n",
        "\n",
        "            # Ensure model is in FP32 for saving\n",
        "            unet_to_save = unet.float() if config.mixed_precision else unet\n",
        "\n",
        "            # Save model components\n",
        "            unet_to_save.save_pretrained(os.path.join(checkpoint_path, \"unet\"))\n",
        "            tokenizer.save_pretrained(os.path.join(checkpoint_path, \"tokenizer\"))\n",
        "\n",
        "            # Save training state\n",
        "            torch.save({\n",
        "                \"epoch\": epoch,\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "                \"lr_scheduler\": lr_scheduler.state_dict(),\n",
        "                \"scaler\": scaler.state_dict(),\n",
        "                \"best_psnr\": best_psnr,\n",
        "            }, os.path.join(checkpoint_path, \"training_state.pt\"))\n",
        "\n",
        "            logger.info(f\"Saved checkpoint to {checkpoint_path}\")\n",
        "\n",
        "            if val_psnr > best_psnr:\n",
        "                best_psnr = val_psnr\n",
        "                # Save as best model\n",
        "                best_path = os.path.join(config.output_dir, \"best_model\")\n",
        "                os.makedirs(best_path, exist_ok=True)\n",
        "                unet_to_save.save_pretrained(os.path.join(best_path, \"unet\"))\n",
        "                logger.info(f\"New best model with PSNR: {best_psnr:.2f}\")\n",
        "\n",
        "        # Log epoch results\n",
        "        logger.info(f\"Epoch {epoch + 1}/{config.num_train_epochs} - \"\n",
        "                   f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
        "                   f\"Val PSNR: {val_psnr:.2f}, Val SSIM: {val_ssim:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during epoch {epoch}: {str(e)}\")\n",
        "        # Save emergency checkpoint\n",
        "        emergency_path = os.path.join(config.output_dir, f\"emergency_checkpoint_epoch_{epoch}\")\n",
        "        os.makedirs(emergency_path, exist_ok=True)\n",
        "        try:\n",
        "            unet_to_save = unet.float() if config.mixed_precision else unet\n",
        "            unet_to_save.save_pretrained(os.path.join(emergency_path, \"unet\"))\n",
        "            torch.save({\n",
        "                \"epoch\": epoch,\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "                \"lr_scheduler\": lr_scheduler.state_dict(),\n",
        "                \"scaler\": scaler.state_dict(),\n",
        "                \"best_psnr\": best_psnr,\n",
        "                \"error\": str(e)\n",
        "            }, os.path.join(emergency_path, \"training_state.pt\"))\n",
        "            logger.info(f\"Saved emergency checkpoint to {emergency_path}\")\n",
        "        except Exception as save_error:\n",
        "            logger.error(f\"Failed to save emergency checkpoint: {str(save_error)}\")\n",
        "\n",
        "        raise e  # Re-raise the original error\n",
        "\n",
        "logger.info(\"Training complete!\")"
      ],
      "metadata": {
        "id": "PfmKroKbq6pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View training results and metrics\n",
        "print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Summary of training\n",
        "print(f\"Best PSNR achieved: {best_psnr:.4f}\")\n",
        "print(f\"Final training loss: {train_loss:.6f}\")\n",
        "print(f\"Final validation loss: {val_loss:.6f}\")\n",
        "print(f\"Final validation PSNR: {val_psnr:.4f}\")\n",
        "print(f\"Final validation SSIM: {val_ssim:.6f}\")"
      ],
      "metadata": {
        "id": "_htnCOGg6q-p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}